---
title: "243 Homework 6"
author: "Ariel Sim (50% of work) & Samantha Maillie (50% of work)"
date: "6/4/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

This problem was very straightfoward particularly given the textbook's walk through with the code. We see an estimated correlation coefficent of .5459189. We later find that the bootstrap produces a smaller standard error than the jackknife. The t-interval approach was a little wider than the others and as a result it crossed over zero and contained negative values. This doesn't give us much insight into the time of correlation betweent the two variables. 

We ran more than just the normal theory confidence intervals. We also ran basic and percentile. Using these three  we get confidence intervals that are entirely positive. This makes intuitive sense given we are examining a relationship bewtween LSAT scores and gpa. However, they do drop down to as low as .1 for a lower bound and rise to around .9 for an upper bound. This indicates a positive correlation but gives little insight into the strength of that relationship. See Hw6 01 code for plots and additional information.

# Problem 2

##a. 

The distribution of $\hat{\theta}_{MLE}$ is the distribution of the maximum order statistic. We know the formula for this is $nf(x)F(x)^{n-1}$ plugging in the appropriate values we get that $\hat{\theta}_{MLE}$ follows the distribution $\frac{n}{\theta^n}x_{n}^{n-1}$.

##b. 
$Var(X_{n}) = E[X_{n}^2] - E[X_{n}]^2$\newline

$E[X_{n}^2] = \int_{0}^{3}\frac{n}{3^n}x_{n}^{n-1}x_{n}^{2}dx_{n}$ \newline
$E[X_{n}^2] = \frac{n}{3^n}\int_{0}^{3}x_{n}^{n+1}dx_{n}$ \newline
$E[X_{n}^2] = \frac{n}{3^n}\frac{1}{n+2}3^{n+2}$ \newline
$E[X_{n}^2] = \frac{9n}{n+2}$ \newline

$E[X_{n}] = \int_{0}^{3}\frac{n}{3^n}x_{n}^{n-1}x_{n}dx_{n}$ \newline
$E[X_{n}] = \frac{n}{3^n}\int_{0}^{3}x_{n}^{n}dx_{n}$ \newline
$E[X_{n}] = \frac{n}{3^n}\frac{1}{n+1}3^{n+1}$ \newline
$E[X_{n}] = \frac{3n}{n+1}$ \newline

$Var(X_{n}) = \frac{9n}{n+2} - (\frac{3n}{n+1})^2$\newline
If we let $n = 50$ we find the variance to be approximately equal to .003327. 

## discussion for c - f
We get a consistantly excellent estimation of this using parametric bootstraping methods. The last run we did gave an estimate of 0.003017126
 although we have seen it vary a little bit. The histogram of estimated $\hat{\theta^*}$'s matches very nicely with the true MLE function plot. 
 
 We see that the nonparametric estimate fails. Since we sample and recalculate the theta value based on the generated values and then regenerate simulation data we think this is causing the issue. The MLE for theta from the uniform distribution with theta as a parameter is always a biased estimator that underestimates theta. We see quickly that as we recalculate theta it goes to zero and hence the method doesn't work. This distribution tends to have difficulties in many applications that work well for most of the other common distributions. 
 
 Plots are contained in the hw6 02 r code file. 



# Problem 3
## a
Because the true function (test function 1) in HW#2 is a piecewise constant, using a genetic algorithm on this test function would provide more reasonable regression curve estimates than applying it on the 2nd test function provided in this problem. This is because the 2nd function is smooth, so approximating using a piecewise function will not produce good results. Plots of both functions can be found attached below. 

## NOTE TO SELF, add plots once the algorithm is done.

## b
The confidence bands for both functions are found using bootstrapping residuals and pairs. The resulting plots are shown below.

## NOTE TO SELF, add plots once the algorithm is done!!!

From the plots, we noticed that bootstrapping residuals' confidence bands are closer to the estimated piecewise function. This is not surprising since this method only resamples the estimated residuals and not the jump points.

As such, we see that the confidence bands obtained from the bootstrapping pairs are more volatile and in some sense wider than the bootstrap residuals, especially around the jump points. Perhaps this method is not as stable since there is a possibility of resampling the same pair.

As for the test function from HW#2, the true function is a smooth function whereas the approximation is piecewise, so the confidence bands themselves will not contain the true function either, thus a poorer approxmation.

## c
The steps are as follows:
1. Create 2000 bootstrap samples
2. Find the best chromosome using the genetic algorithm for each of the bootstrapped samples
3. Count the $n_i$'s that are selected as jump points for each $i$
4. For confidence level $1-\alpha$, take the smallest set of points where the proportion of the sum of their jump points counts are at least $1-\alpha$.

# Problem 4

## a. 

$likelihood = \prod \lambda e^{-\lambda x_{i}}$ \newline
$likelihood = \lambda^n e^{-\lambda \sum x_{i}}$ \newline
$L =log-likelihood = nlog(\lambda) -\lambda \sum x_{i}$ \newline
$L' = \frac{n}{\lambda} -\sum x_{i} = 0$ \newline
$\hat{\lambda}_{MLE} = \frac{n}{\sum x_{i}} = \frac{1}{\bar{X}}$ \newline


## b for part 1 see the picture below.
$\sqrt{n}(\hat{\lambda}_{n} - \lambda) \rightarrow^{d} N(0,\lambda^2)$
\newline
![Proof for 4b.](243_4b.jpg)

$\sqrt{n}(log(\hat{\lambda}_{n}) - log(\lambda)) \rightarrow^{d} N(0,1)$
\newline
From the above we know that applying the log function will result in the mean remaining as zero. We also know from the delta method that our new variance will be $(g'(\lambda))^2Var(\lambda)$ where $g(\lambda) = log(\lambda)$ and therefore $g'(\lambda) = \frac{1}{\lambda}$ Putting it all together we get the $g'(\lambda)^2$ and $Var(\lambda)$ cancelling each other out so that we have a variance of 1. 

## c See the picture below.

![Proof for 4c.](243_4c.jpg)

## d See the picture below. 

![Proof for 4d.](243_4d.jpg)

## e

## References
1.Rizzo, Maria L. Statistical Computing with R. Chapman & Hall/CRC, 2017.
